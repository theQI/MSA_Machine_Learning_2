{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "Step1. Warm-up \n",
    "Step2. Implement OCR code in Tensorflow Add new layer Drop out Step 3. Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the response and explain what tf.Session() and tf.constant() commands do.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. Implement OCR code in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0825 12:53:46.504951  7860 deprecation.py:323] From <ipython-input-2-93d8da72a918>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0825 12:53:46.507950  7860 deprecation.py:323] From C:\\Users\\aks23\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0825 12:53:46.511948  7860 deprecation.py:323] From C:\\Users\\aks23\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0825 12:53:47.820199  7860 deprecation.py:323] From C:\\Users\\aks23\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0825 12:53:49.025511  7860 deprecation.py:323] From C:\\Users\\aks23\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0825 12:53:49.036507  7860 deprecation.py:323] From C:\\Users\\aks23\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0825 12:53:49.905009  7860 deprecation.py:323] From C:\\Users\\aks23\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1- Explain this portion of the code.\n",
    "#Q2- what is the usage of one_hot in read_data_sets() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that can define the size of train and test sets\n",
    "def TRAIN_SIZE(num):\n",
    "    print ('Total Training Images in Dataset = ' + str(mnist.train.images.shape))\n",
    "    print ('--------------------------------------------------')\n",
    "    x_train = mnist.train.images[:num,:]\n",
    "    print ('x_train Examples Loaded = ' + str(x_train.shape))\n",
    "    y_train = mnist.train.labels[:num,:]\n",
    "    print ('y_train Examples Loaded = ' + str(y_train.shape))\n",
    "    print('')\n",
    "    return x_train, y_train\n",
    "\n",
    "def TEST_SIZE(num):\n",
    "    print ('Total Test Examples in Dataset = ' + str(mnist.test.images.shape))\n",
    "    print ('--------------------------------------------------')\n",
    "    x_test = mnist.test.images[:num,:]\n",
    "    print ('x_test Examples Loaded = ' + str(x_test.shape))\n",
    "    y_test = mnist.test.labels[:num,:]\n",
    "    print ('y_test Examples Loaded = ' + str(y_test.shape))\n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random as ran\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_digit(num):\n",
    "    print(y_train[num])\n",
    "    label = y_train[num].argmax(axis=0)\n",
    "    image = x_train[num].reshape([28,28])\n",
    "    plt.title('Example: %d Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mult_flat(start, stop):\n",
    "    images = x_train[start].reshape([1,784])\n",
    "    for i in range(start+1,stop):\n",
    "        images = np.concatenate((images, x_train[i].reshape([1,784])))\n",
    "    plt.imshow(images, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Images in Dataset = (55000, 784)\n",
      "--------------------------------------------------\n",
      "x_train Examples Loaded = (55000, 784)\n",
      "y_train Examples Loaded = (55000, 10)\n",
      "\n",
      "Total Test Examples in Dataset = (10000, 784)\n",
      "--------------------------------------------------\n",
      "x_test Examples Loaded = (10000, 784)\n",
      "y_test Examples Loaded = (10000, 10)\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train = TRAIN_SIZE(55000)\n",
    "\n",
    "x_test, y_test = TEST_SIZE(10000)\n",
    "\n",
    "display_digit(ran.randint(0, x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# Input and output\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # input image 28*28 = 784\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10]) # 0-9 digits (10 classes)\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3- It appears that tf.placeholder() , tf.Variable() and tf.constant() all are for assigning variables so what is the difference between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q.4) Write a few sentences about tf.train.GradientDescentOptimizer and tf.equal commands. What is the effect of tf.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0 Accuracy = 0.6705 Loss = 2.1970634\n",
      "Training Step:100 Accuracy = 0.8713 Loss = 0.6085837\n",
      "Training Step:200 Accuracy = 0.8857 Loss = 0.49071\n"
     ]
    }
   ],
   "source": [
    "TRAIN_STEPS = 250\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={x: x_train, y_: y_train})\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + ' Accuracy = ' +\n",
    "    str(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})) + ' Loss = ' +\n",
    "    str(sess.run(cross_entropy, {x: x_train, y_: y_train})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5- Explain this part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # input image 28*28 = 784\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10]) #0-9 digits (10 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two hidden layers, output layer and their number of neurons (the last layer has 10 softmax neurons)\n",
    "L = 100\n",
    "M = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.truncated_normal is a TensorFlow function that produces random values following the normal (Gaussian) distribution between -2*stddev and +2*stddev\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1)) # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.zeros([L]))\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([M]))\n",
    "W3 = tf.Variable(tf.truncated_normal([M, 10], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
    "Ylogits = tf.matmul(Y2, W3) + B3\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0825 13:05:41.907672  7860 deprecation.py:323] From <ipython-input-16-16e072c4eb32>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100 images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits,labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0 Accuracy = 0.101 Loss = 375.5443\n",
      "Training Step:100 Accuracy = 0.4983 Loss = 162.48523\n",
      "Training Step:200 Accuracy = 0.9259 Loss = 26.892141\n",
      "Training Step:300 Accuracy = 0.9269 Loss = 27.17197\n",
      "Training Step:400 Accuracy = 0.952 Loss = 14.366916\n",
      "Training Step:500 Accuracy = 0.9604 Loss = 10.672213\n",
      "Training Step:600 Accuracy = 0.9499 Loss = 16.726273\n",
      "Training Step:700 Accuracy = 0.9624 Loss = 9.389396\n",
      "Training Step:800 Accuracy = 0.9649 Loss = 7.374257\n",
      "Training Step:900 Accuracy = 0.966 Loss = 6.0084867\n",
      "Training Step:1000 Accuracy = 0.943 Loss = 20.963127\n"
     ]
    }
   ],
   "source": [
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "#correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "TRAIN_STEPS = 1000\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={X: x_train, Y_: y_train})\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + ' Accuracy = ' +\n",
    "    str(sess.run(accuracy, feed_dict={X: x_test, Y_: y_test})) + ' Loss = ' +\n",
    "    str(sess.run(cross_entropy, {X: x_train, Y_: y_train})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out\n",
    "Tensorflow can make dropout algorithm. By default it can keep 75% of the nodes during the\n",
    "trainin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0825 13:18:33.568734  7860 deprecation.py:506] From <ipython-input-20-e0105bfe7cc1>:19: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Input and output\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # input image 28*28 = 784\n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10]) #0-9 digits (10 classes)\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "L = 100\n",
    "M = 30\n",
    "N = 60\n",
    "O = 30\n",
    "# tf.truncated_normal is a TensorFlow function that produces random values following the normal (Gaussian) distribution between -2*stddev and +2*stddev\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1)) # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.ones([L])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([M])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([M, 10], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "# The model\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + B2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "Ylogits = tf.matmul(Y2d, W3) + B3\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:0 Accuracy = 0.1726Loss = 224.33026\n",
      "Training Step:100 Accuracy = 0.908Loss = 40.207478\n",
      "Training Step:200 Accuracy = 0.9481Loss = 22.996412\n",
      "Training Step:300 Accuracy = 0.9595Loss = 17.545042\n",
      "Training Step:400 Accuracy = 0.9623Loss = 14.517075\n",
      "Training Step:500 Accuracy = 0.9662Loss = 12.436522\n",
      "Training Step:600 Accuracy = 0.9668Loss = 10.724318\n",
      "Training Step:700 Accuracy = 0.9684Loss = 9.661769\n",
      "Training Step:800 Accuracy = 0.9708Loss = 8.318644\n",
      "Training Step:900 Accuracy = 0.9725Loss = 7.4137015\n",
      "Training Step:1000 Accuracy = 0.9727Loss = 6.877028\n"
     ]
    }
   ],
   "source": [
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100 images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "TRAIN_STEPS = 1000\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={X: x_train, Y_: y_train, pkeep: 0.75 })\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + ' Accuracy = ' +\n",
    "    str(sess.run(accuracy, feed_dict={X: x_test, Y_: y_test, pkeep: 1.0})) + \n",
    "    'Loss = ' + str(sess.run(cross_entropy, {X: x_train, Y_: y_train , pkeep: 0.75})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
